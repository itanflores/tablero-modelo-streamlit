import streamlit as st
import pandas as pd
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import requests
import io

# ğŸ› ï¸ ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(page_title="Tablero de PrecisiÃ³n del Modelo", page_icon="ğŸ“Š", layout="wide")

# ğŸ“Œ TÃ­tulo del tablero
st.title("ğŸ“Š Tablero de EvaluaciÃ³n del Modelo de ClasificaciÃ³n")

# ğŸ“Œ Cargar Dataset desde GitHub con Git LFS
GITHUB_API_URL = "https://github.com/itanflores/tablero-modelo-streamlit/raw/main/dataset_monitoreo_servers.csv"
response = requests.get(GITHUB_API_URL, stream=True)

if response.status_code == 200:
    df = pd.read_csv(io.BytesIO(response.content), encoding="utf-8")
else:
    st.error("âŒ Error al descargar el dataset desde GitHub.")
    st.stop()

df.columns = df.columns.str.strip()

# ğŸ“Œ Verificar si la columna correcta existe
if "Estado del Sistema" in df.columns:
    df['Estado del Sistema Codificado'] = df['Estado del Sistema'].map({"Inactivo": 0, "Normal": 1, "Advertencia": 2, "CrÃ­tico": 3})
else:
    st.error("âŒ Error: La columna 'Estado del Sistema' no se encuentra en el dataset.")
    st.stop()

# ğŸ“Œ Preprocesamiento de Datos
X = df.drop(["Estado del Sistema", "Estado del Sistema Codificado"], axis=1)
y = df["Estado del Sistema Codificado"]

# Convertir datos categÃ³ricos en variables numÃ©ricas si existen
X = pd.get_dummies(X, drop_first=True)
X = X.reindex(columns=X.columns, fill_value=0)  # Asegurar misma estructura en entrenamiento y prueba

# Normalizar los datos para mejorar la estabilidad del modelo
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# OptimizaciÃ³n: Reducir el tamaÃ±o del dataset
X = X.astype(np.float32)
y = y.astype(np.int8)

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# ğŸ”¹ SecciÃ³n 1: EvaluaciÃ³n General del Modelo
st.header("ğŸ“Œ EvaluaciÃ³n General del Modelo")

# Entrenar modelo con hiperparÃ¡metros ajustados
model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# MÃ©tricas de evaluaciÃ³n
accuracy = accuracy_score(y_test, y_pred)
st.metric("PrecisiÃ³n del Modelo", f"{accuracy:.4f}")
st.text("Reporte de ClasificaciÃ³n:")
st.text(classification_report(y_test, y_pred))

# Matriz de ConfusiÃ³n
conf_matrix = confusion_matrix(y_test, y_pred)
st.subheader("ğŸ“Š Matriz de ConfusiÃ³n")
fig, ax = plt.subplots(figsize=(5, 3))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Inactivo", "Normal", "Advertencia", "CrÃ­tico"], yticklabels=["Inactivo", "Normal", "Advertencia", "CrÃ­tico"])
plt.xlabel("PredicciÃ³n")
plt.ylabel("Real")
st.pyplot(fig)

# ğŸ”¹ SecciÃ³n 2: Importancia de Variables
df_importance = pd.DataFrame({"Variable": X.columns, "Importancia": model.feature_importances_}).sort_values(by="Importancia", ascending=False)
st.header("ğŸ“Š Importancia de Variables en la PredicciÃ³n")
st.plotly_chart(px.bar(df_importance.head(10), x="Importancia", y="Variable", orientation='h', title="ğŸ“Š Importancia de Variables"), use_container_width=True)

# ğŸ”¹ SecciÃ³n 3: ComparaciÃ³n de Modelos en Tabla
st.header("ğŸ“Š ComparaciÃ³n de Modelos de ClasificaciÃ³n")
model_scores = {"Random Forest": accuracy}

# Evaluar otros modelos de forma opcional
if st.checkbox("Comparar con otros modelos"):
    from sklearn.linear_model import LogisticRegression
    from sklearn.tree import DecisionTreeClassifier
    models = {"RegresiÃ³n LogÃ­stica": LogisticRegression(max_iter=200), "Ãrbol de DecisiÃ³n": DecisionTreeClassifier(max_depth=5)}
    for name, clf in models.items():
        try:
            clf.fit(X_train, y_train)
            model_scores[name] = accuracy_score(y_test, clf.predict(X_test))
        except Exception as e:
            model_scores[name] = f"Error: {str(e)}"
    
    # Mostrar tabla en lugar de grÃ¡fico
    df_scores = pd.DataFrame.from_dict(model_scores, orient='index', columns=["PrecisiÃ³n Promedio"]).reset_index()
    df_scores.rename(columns={"index": "Modelo"}, inplace=True)
    st.dataframe(df_scores)
