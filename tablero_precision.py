import streamlit as st
import pandas as pd
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
import requests
import io

# ğŸ› ï¸ ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(page_title="Tablero de PrecisiÃ³n del Modelo", page_icon="ğŸ“Š", layout="wide")

# ğŸ“¢ TÃ­tulo del tablero
st.title("ğŸ“Š Tablero de EvaluaciÃ³n del Modelo de ClasificaciÃ³n")

# ğŸ“Œ Cargar Dataset desde GitHub con Git LFS
GITHUB_API_URL = "https://github.com/itanflores/tablero-modelo-streamlit/raw/main/dataset_monitoreo_servers.csv"
response = requests.get(GITHUB_API_URL, stream=True)

if response.status_code == 200:
    df = pd.read_csv(io.BytesIO(response.content), encoding="utf-8")
else:
    st.error("âŒ Error al descargar el dataset desde GitHub.")
    st.stop()

df.columns = df.columns.str.strip()

# ğŸ“Œ Verificar si la columna correcta existe
if "Estado del Sistema" in df.columns:
    df['Estado del Sistema Codificado'] = df['Estado del Sistema'].map({"Inactivo": 0, "Normal": 1, "Advertencia": 2, "CrÃ­tico": 3})
else:
    st.error("âŒ Error: La columna 'Estado del Sistema' no se encuentra en el dataset.")
    st.stop()

# ğŸ“Œ Preprocesamiento de Datos
X = df.drop(["Estado del Sistema", "Estado del Sistema Codificado"], axis=1)
y = df["Estado del Sistema Codificado"]

# Convertir datos categÃ³ricos en variables numÃ©ricas si existen
X = pd.get_dummies(X, drop_first=True)

# Dividir los datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# ğŸ”¹ SecciÃ³n 1: EvaluaciÃ³n General del Modelo
st.header("ğŸ“Œ EvaluaciÃ³n General del Modelo")

# Entrenar modelo
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# MÃ©tricas de evaluaciÃ³n
accuracy = accuracy_score(y_test, y_pred)
st.metric("PrecisiÃ³n del Modelo", f"{accuracy:.4f}")
st.text("Reporte de ClasificaciÃ³n:")
st.text(classification_report(y_test, y_pred))

# Matriz de ConfusiÃ³n
conf_matrix = confusion_matrix(y_test, y_pred)
st.subheader("ğŸ“Š Matriz de ConfusiÃ³n")
fig, ax = plt.subplots(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["Inactivo", "Normal", "Advertencia", "CrÃ­tico"], yticklabels=["Inactivo", "Normal", "Advertencia", "CrÃ­tico"])
plt.xlabel("PredicciÃ³n")
plt.ylabel("Real")
st.pyplot(fig)

# ğŸ”¹ SecciÃ³n 2: Importancia de Variables
st.header("ğŸ“Œ Importancia de Variables en la PredicciÃ³n")
importances = model.feature_importances_
feature_importance_df = pd.DataFrame({"Variable": X.columns, "Importancia": importances}).sort_values(by="Importancia", ascending=False)
st.plotly_chart(px.bar(feature_importance_df, x="Importancia", y="Variable", orientation='h', title="ğŸ“Š Importancia de Variables"), use_container_width=True)

# ğŸ”¹ SecciÃ³n 3: ComparaciÃ³n de Modelos
st.header("ğŸ“Œ ComparaciÃ³n de Modelos de ClasificaciÃ³n")
model_scores = {"Random Forest": accuracy}

# Evaluar otros modelos
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
models = {"RegresiÃ³n LogÃ­stica": LogisticRegression(max_iter=500), "Ãrbol de DecisiÃ³n": DecisionTreeClassifier()}
for name, clf in models.items():
    clf.fit(X_train, y_train)
    model_scores[name] = accuracy_score(y_test, clf.predict(X_test))

# VisualizaciÃ³n de comparaciÃ³n
df_scores = pd.DataFrame.from_dict(model_scores, orient='index', columns=["PrecisiÃ³n"]).reset_index()
st.plotly_chart(px.bar(df_scores, x="PrecisiÃ³n", y="index", orientation='h', title="ğŸ“Š PrecisiÃ³n de Modelos"), use_container_width=True)

# ğŸ”¹ SecciÃ³n 4: Curva ROC
st.header("ğŸ“Œ Curvas ROC/AUC")
y_test_binarized = label_binarize(y_test, classes=[0, 1, 2, 3])
y_score = model.predict_proba(X_test)
fpr, tpr, roc_auc = {}, {}, {}

fig_roc, ax_roc = plt.subplots(figsize=(8, 6))
colors = ["blue", "green", "orange", "red"]
for i, color in enumerate(colors):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    ax_roc.plot(fpr[i], tpr[i], color=color, lw=2, label=f"Clase {i} (AUC = {roc_auc[i]:.2f})")
ax_roc.plot([0, 1], [0, 1], color="gray", linestyle="--", lw=2)
ax_roc.set_title("Curvas ROC por Clase")
ax_roc.set_xlabel("Tasa de Falsos Positivos")
ax_roc.set_ylabel("Tasa de Verdaderos Positivos")
ax_roc.legend(loc="lower right")
st.pyplot(fig_roc)
