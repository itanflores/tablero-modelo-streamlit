import streamlit as st
import pandas as pd
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import requests
import boto3
import io

# ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(page_title="Tablero de ClasificaciÃ³n en Streamlit para la GestiÃ³n Predictiva de Infraestructura TI", page_icon="ğŸ“Š", layout="wide")

# ğŸ“Œ TÃ­tulo
st.title("ğŸ“Š Tablero de ClasificaciÃ³n en Streamlit para la GestiÃ³n Predictiva de Infraestructura TI")

# Configurar S3
BUCKET_NAME = "tfm-monitoring-data"
FILE_NAME = "dataset_monitoreo_servers.csv"

# Crear cliente S3
s3 = boto3.client("s3")

try:
    obj = s3.get_object(Bucket=BUCKET_NAME, Key=FILE_NAME)
    df = pd.read_csv(io.BytesIO(obj["Body"].read()), encoding="utf-8")
    df.columns = df.columns.str.strip()
except boto3.exceptions.Boto3Error as e:
    st.error(f"ğŸš¨ Error al conectar con S3: {e}")
    st.stop()
except Exception as e:
    st.error(f"âŒ Error al descargar el dataset desde S3: {e}")
    st.stop()

df.columns = df.columns.str.strip()

# Verificar columna objetivo
if "Estado del Sistema" in df.columns:
    df['Estado del Sistema Codificado'] = df['Estado del Sistema'].map({"Inactivo": 0, "Normal": 1, "Advertencia": 2, "CrÃ­tico": 3})
else:
    st.error("âŒ Error: La columna 'Estado del Sistema' no se encuentra.")
    st.stop()

# Preprocesamiento
X = df.drop(["Estado del Sistema", "Estado del Sistema Codificado"], axis=1)
y = df["Estado del Sistema Codificado"]
X = pd.get_dummies(X, drop_first=True)
scaler = StandardScaler()
X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Entrenamiento del modelo
model = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# ğŸ“Œ **SecciÃ³n 1: EvaluaciÃ³n General del Modelo**
st.header("ğŸ“Œ EvaluaciÃ³n General del Modelo")

col1, col2, col3 = st.columns([1.5, 2, 2])

with col1:
    st.metric("ğŸ“Š PrecisiÃ³n del Modelo", f"{accuracy_score(y_test, y_pred):.4f}")
    st.caption("ğŸ”¹ La precisiÃ³n mide la proporciÃ³n de predicciones correctas. Valores mÃ¡s altos indican mejor desempeÃ±o.")

with col2.expander("ğŸ“‹ Reporte de ClasificaciÃ³n"):
    st.text(classification_report(y_test, y_pred))
    st.caption("ğŸ”¹ El reporte muestra mÃ©tricas clave como precisiÃ³n, recall y F1-score para evaluar el desempeÃ±o en cada categorÃ­a.")

with col3:
    st.write("ğŸ“Š Matriz de ConfusiÃ³n")
    fig, ax = plt.subplots(figsize=(5, 4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
    st.pyplot(fig)
    st.caption("ğŸ”¹ La matriz de confusiÃ³n muestra las predicciones correctas en la diagonal y los errores fuera de ella.")

st.divider()

# ğŸ“Œ **SecciÃ³n 2: Importancia de Variables**
st.header("ğŸ“Š Importancia de Variables en la PredicciÃ³n")

df_importance = pd.DataFrame({
    "Variable": X.columns,
    "Importancia": model.feature_importances_
}).sort_values(by="Importancia", ascending=False)

col4, col5 = st.columns([1.5, 3])

with col4.expander("ğŸ“‹ Ver Variables Importantes", expanded=True):
    st.dataframe(df_importance.head(10), height=300)
    st.caption("ğŸ”¹ Muestra las 10 variables mÃ¡s importantes utilizadas por el modelo para la toma de decisiones.")

fig_imp = px.bar(df_importance.head(10), 
                 x="Importancia", 
                 y="Variable", 
                 orientation='h', 
                 title="ğŸ“Š Importancia de Variables")

col5.plotly_chart(fig_imp, use_container_width=True)
st.caption("ğŸ”¹ Este grÃ¡fico destaca quÃ© variables tienen mayor peso en las predicciones del modelo.")

st.divider()

# ğŸ“Œ **SecciÃ³n 3: ComparaciÃ³n de Modelos**
st.header("ğŸ“Š ComparaciÃ³n de Modelos de ClasificaciÃ³n")

# ğŸ”¹ PestaÃ±as para organizar cada modelo
tab1, tab2, tab3 = st.tabs(["ğŸŒ³ Ãrbol de DecisiÃ³n", "ğŸ“ˆ RegresiÃ³n LogÃ­stica", "ğŸŒ² Random Forest"])

# ğŸ“Œ SecciÃ³n Ãrbol de DecisiÃ³n en `st.tabs()`
with tab1:
    st.subheader("ğŸŒ³ Ãrbol de DecisiÃ³n")
    st.caption("ğŸ”¹ Modelo basado en reglas jerÃ¡rquicas. Es fÃ¡cil de interpretar pero puede sobreajustarse con demasiada profundidad.")

    if st.button("Entrenar Ãrbol de DecisiÃ³n"):
        with st.spinner("Entrenando..."):
            from sklearn.tree import DecisionTreeClassifier
            tree_clf = DecisionTreeClassifier(max_depth=5)
            tree_clf.fit(X_train, y_train)
            st.session_state["modelo_entrenado"] = tree_clf  # Guardar el modelo entrenado
            st.session_state["tree_acc"] = accuracy_score(y_test, tree_clf.predict(X_test))
            st.session_state["tree_cm"] = confusion_matrix(y_test, tree_clf.predict(X_test))
            st.session_state["tree_trained"] = True

    if st.session_state.get("tree_trained", False):
        st.metric("PrecisiÃ³n", f"{st.session_state['tree_acc']:.4f}")

# ğŸ“ˆ **RegresiÃ³n LogÃ­stica**
with tab2:
    st.subheader("ğŸ“ˆ RegresiÃ³n LogÃ­stica")
    st.caption("ğŸ”¹ Modelo lineal utilizado para clasificaciones binarias o multiclase con buena interpretabilidad.")

    if st.button("Entrenar RegresiÃ³n LogÃ­stica"):
        with st.spinner("Entrenando..."):
            from sklearn.linear_model import LogisticRegression
            log_clf = LogisticRegression(max_iter=50, n_jobs=-1)  # Menos iteraciones y paralelizaciÃ³n
            log_clf.fit(X_train, y_train)
            st.session_state["modelo_entrenado"] = log_clf  # Guardar el modelo entrenado
            acc_log = accuracy_score(y_test, log_clf.predict(X_test))
            st.metric("PrecisiÃ³n", f"{acc_log:.4f}")

            # ğŸ“Š Mostrar matriz de confusiÃ³n
            fig, ax = plt.subplots(figsize=(5, 4))
            sns.heatmap(confusion_matrix(y_test, log_clf.predict(X_test)), annot=True, fmt="d", cmap="Blues")
            st.pyplot(fig)
            st.caption("ğŸ”¹ La matriz de confusiÃ³n evalÃºa quÃ© tan bien el modelo distingue entre clases.")
            
# ğŸŒ² **Random Forest**
with tab3:
    st.subheader("ğŸŒ² Random Forest")
    st.caption("ğŸ”¹ Conjunto de mÃºltiples Ã¡rboles de decisiÃ³n que mejora la precisiÃ³n y reduce el sobreajuste.")

    if st.button("Entrenar Random Forest"):
        with st.spinner("Entrenando..."):
            forest_clf = RandomForestClassifier(
                n_estimators=50,  # Menos Ã¡rboles
                max_depth=10,     # Limitar la profundidad
                max_samples=0.5,  # Usar solo el 50% de los datos para cada Ã¡rbol
                n_jobs=-1,        # ParalelizaciÃ³n
                random_state=42
            )
            forest_clf.fit(X_train, y_train)
            st.session_state["modelo_entrenado"] = forest_clf  # Guardar el modelo entrenado
            acc_forest = accuracy_score(y_test, forest_clf.predict(X_test))
            st.metric("PrecisiÃ³n", f"{acc_forest:.4f}")

            # ğŸ“Š Mostrar matriz de confusiÃ³n
            fig, ax = plt.subplots(figsize=(5, 4))
            sns.heatmap(confusion_matrix(y_test, forest_clf.predict(X_test)), annot=True, fmt="d", cmap="Blues")
            st.pyplot(fig)
            st.caption("ğŸ”¹ EvalÃºa los aciertos y errores del modelo en la clasificaciÃ³n.")

# ğŸ”¹ Nueva SecciÃ³n: Curva ROC y AUC
st.header("ğŸ“ˆ Curva ROC y AUC")

# Verificar si hay un modelo entrenado
if "modelo_entrenado" in st.session_state:
    try:
        # Obtener el modelo entrenado
        modelo = st.session_state["modelo_entrenado"]

        # Obtener las probabilidades predichas para cada clase
        y_pred_proba = modelo.predict_proba(X_test)

        # Calcular la Curva ROC y el AUC para cada clase
        from sklearn.metrics import roc_curve, auc
        from sklearn.preprocessing import label_binarize
        import plotly.graph_objects as go

        # Binarizar las etiquetas para multiclase
        y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])
        n_classes = y_test_bin.shape[1]

        # Calcular la Curva ROC y el AUC para cada clase
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        for i in range(n_classes):
            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        # Crear la grÃ¡fica de la Curva ROC
        fig_roc = go.Figure()
        for i in range(n_classes):
            fig_roc.add_trace(go.Scatter(
                x=fpr[i],
                y=tpr[i],
                mode='lines',
                name=f'Clase {i} (AUC = {roc_auc[i]:.2f})'
            ))

        # AÃ±adir lÃ­nea de referencia (clasificador aleatorio)
        fig_roc.add_trace(go.Scatter(
            x=[0, 1],
            y=[0, 1],
            mode='lines',
            line=dict(dash='dash'),
            name='Clasificador Aleatorio (AUC = 0.5)'
        ))

        # Configurar el diseÃ±o del grÃ¡fico
        fig_roc.update_layout(
            title='Curva ROC Multiclase',
            xaxis_title='Tasa de Falsos Positivos (FPR)',
            yaxis_title='Tasa de Verdaderos Positivos (TPR)',
            legend_title="Clases",
            width=800,
            height=600
        )

        # Mostrar la grÃ¡fica
        st.plotly_chart(fig_roc, use_container_width=True)
        st.write("""
        La **Curva ROC** muestra el rendimiento del modelo en la clasificaciÃ³n multiclase.
        - **AUC (Ãrea Bajo la Curva)**: Un valor cercano a 1 indica un modelo excelente, mientras que un valor cercano a 0.5 sugiere que el modelo no es mejor que una predicciÃ³n aleatoria.
        """)
    except Exception as e:
        st.error(f"âŒ Error al calcular la Curva ROC: {e}")
else:
    st.warning("âš ï¸ Advertencia: No se ha entrenado ningÃºn modelo. Entrena un modelo primero para calcular la Curva ROC.")
