#  Secci贸n 2: Secci贸n de Pron贸sticos
# [C贸digo existente de la Secci贸n 2]

#  Nueva Secci贸n: Evaluaci贸n del Modelo
st.header(" Evaluaci贸n del Modelo")

# Calcular la Curva ROC y el AUC
y_true = df_filtrado["Estado del Sistema"].apply(lambda x: 1 if x == "Cr铆tico" else 0)
y_pred_proba = model.predict_proba(df_filtrado[["Uso CPU (%)", "Temperatura (掳C)"]])[:, 1]

fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)
roc_auc = auc(fpr, tpr)

# Crear la gr谩fica de la Curva ROC
fig_roc = px.area(
    x=fpr,
    y=tpr,
    title=f'Curva ROC (AUC = {roc_auc:.2f})',
    labels=dict(x='Tasa de Falsos Positivos (FPR)', y='Tasa de Verdaderos Positivos (TPR)'),
    width=700, height=500
)
fig_roc.add_shape(
    type='line', line=dict(dash='dash'),
    x0=0, x1=1, y0=0, y1=1
)
fig_roc.update_yaxes(scaleanchor="x", scaleratio=1)
fig_roc.update_xaxes(constrain='domain')

# Mostrar la gr谩fica
st.plotly_chart(fig_roc, use_container_width=True)
st.write("""
La **Curva ROC** muestra el rendimiento de un modelo de clasificaci贸n en todos los umbrales de clasificaci贸n.
- **AUC (rea Bajo la Curva)**: Un valor cercano a 1 indica un modelo excelente, mientras que un valor cercano a 0.5 sugiere que el modelo no es mejor que una predicci贸n aleatoria.
""")

#  Secci贸n 3: An谩lisis de Outliers y Eficiencia T茅rmica
# [C贸digo existente de la Secci贸n 3]
